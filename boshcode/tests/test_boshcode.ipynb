{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../../cnn_design-space/cnnbench/')\n",
    "sys.path.append('../../boshnas/boshnas/')\n",
    "sys.path.append('../')\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import yaml\n",
    "import random\n",
    "import tabulate\n",
    "import subprocess\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "from six.moves import cPickle as pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from boshnas_2inp import BOSHNAS as BOSHCODE\n",
    "from acq import gosh_acq as acq\n",
    "\n",
    "from run_boshcode import convert_to_tabular, update_dataset, get_neighbor_hash\n",
    "\n",
    "from library import GraphLib, Graph\n",
    "from utils import print_util as pu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained CNN-Accelerator pair with hash: 293181b319b0deec6cfc31771b3c1464fe538f6f93d9eb22b2d73891d50e11fc, not in current dataset\n",
      "CNN-Accelerator pair with hash: d26935273a41e425dbef42f8a61ab57b4fba8483eababe4f044625410030f63a, doesn't have respective CNN trained (with hash: d74c8175e990c815efa1092f254fc65cef658f5051e36b404777a3b3bc31e059)\n",
      "CNN-Accelerator pair with hash: af95420c27b1e19b0a1fb8d1166fb646c8308cf89a34647765c71a9463db85b6, doesn't have respective CNN trained (with hash: a3c500d1cd395f12812fae0db02e72ff3ec9962a0c0e372dc812541f7ba038a2)\n",
      "CNN-Accelerator pair with hash: 95bd07e8360817122001ccb7bb73bb58d82c4bc50560d7ea877ccceede95b1d2, doesn't have respective CNN trained (with hash: d6b07ee2297fa76ab5e71507ca95ab73f9e6e95461cb856ac79b34818751aba8)\n",
      "CNN-Accelerator pair with hash: e883fa4f8988b3f67bd9e26ae334edb44d031a90851d22bb0566d64b0ac6cbec, doesn't have respective CNN trained (with hash: 1abf976cd9b816f2e37532b7cb82dba8732ffea628880c3a716b6f5d5a235b66)\n",
      "CNN-Accelerator pair with hash: 80c612677b7bb53dea8fe012488babc6a8e6547f06d1c5a3de9f13da8a7bfeaa, doesn't have respective CNN trained (with hash: 7d6a305b064b0e0f19d226192f154c64a97caca2bab62b0c352fe60ceae007e3)\n",
      "Trained CNN-Accelerator pair with hash: 5227ff8b5f50e650331c693f9f15d00db6f5eee701c1ebd3724cdb9b98f9ca2a, not in current dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Updating CNN-Accelerator library:   0%|          | 0/41 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mDataset saved to:\u001b[0m ./dataset_mini_trained.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating CNN-Accelerator library: 100%|██████████| 41/41 [00:01<00:00, 35.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[92mTrained CNNs in dataset:\u001b[0m 25\n",
      "\u001b[92mSimulated CNN-Accelerator pairs:\u001b[0m 39\n",
      "\u001b[92mBest performance:\u001b[0m 0.9522054327578187\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting dataset to tabular: 100%|██████████| 85630034/85630034 [00:50<00:00, 1705259.63it/s]\n"
     ]
    }
   ],
   "source": [
    "graphlib_file = '../../cnn_design-space/cnnbench/dataset/dataset_mini.json'\n",
    "new_graphlib_file = './dataset_mini_trained.json'\n",
    "accel_embeddings_file = '../../accelerator_design-space/accelbench/embeddings/embeddings.pkl'\n",
    "cnn_config_file = '../../cnn_design-space/cnnbench/configs/CIFAR10/config.yaml'\n",
    "models_dir = '../../models'\n",
    "accel_dataset_file = '../accel_dataset/accel_dataset_mini_bkp.pkl'\n",
    "accel_dataset_file_trained = './accel_dataset_mini_trained.pkl'\n",
    "performance_weights = [0, 0.2, 0, 0.2, 0.1, 0.2, 0.3]\n",
    "\n",
    "graphLib = GraphLib.load_from_dataset(graphlib_file)\n",
    "\n",
    "# accel_embeddings = pickle.load(open(accel_embeddings_file, 'rb'))\n",
    "# accel_embeddings = np.array(accel_embeddings)\n",
    "\n",
    "cnn_config = yaml.safe_load(open(cnn_config_file))\n",
    "\n",
    "cnn_models_dir = os.path.join(models_dir, 'cnnbench_models', cnn_config['dataset'])\n",
    "accel_models_dir = os.path.join(models_dir, 'accelbench_models')\n",
    "\n",
    "# Get trained CNN models and Accelerator architectures\n",
    "trained_cnn_hashes = os.listdir(cnn_models_dir)\n",
    "trained_cnn_hashes_new = []\n",
    "for cnn_hash in trained_cnn_hashes:\n",
    "    if 'model.pt' in os.listdir(os.path.join(cnn_models_dir, cnn_hash)): trained_cnn_hashes_new.append(cnn_hash)\n",
    "        \n",
    "trained_cnn_hashes = trained_cnn_hashes_new\n",
    "trained_accel_hashes = [accel_hash[:-4] for accel_hash in os.listdir(accel_models_dir)]\n",
    "\n",
    "# Load CNN-Accelerator pairs dataset\n",
    "accel_dataset = pickle.load(open(accel_dataset_file, 'rb'))\n",
    "\n",
    "accel_embeddings = [accel['accel_emb'].tolist() for accel in accel_dataset.values()]\n",
    "accel_embeddings = [str(elem) for elem in accel_embeddings]\n",
    "accel_embeddings = [eval(elem) for elem in set(accel_embeddings)]\n",
    "accel_embeddings = np.array(accel_embeddings)\n",
    "\n",
    "accel_hashes = list(accel_dataset.keys())\n",
    "\n",
    "# Check trained_accel_hashes have all respective CNNs trained\n",
    "trained_accel_hashes_new = []\n",
    "for accel_hash in trained_accel_hashes:\n",
    "    if accel_hash not in accel_hashes:\n",
    "        print(f'Trained CNN-Accelerator pair with hash: {accel_hash}, not in current dataset')\n",
    "        continue\n",
    "    cnn_hash = accel_dataset[accel_hash]['cnn_hash']\n",
    "    if cnn_hash not in trained_cnn_hashes:\n",
    "        print(f'CNN-Accelerator pair with hash: {accel_hash}, doesn\\'t have respective CNN trained (with hash: {cnn_hash})')\n",
    "    else:\n",
    "        trained_accel_hashes_new.append(accel_hash)\n",
    "        \n",
    "trained_accel_hashes = trained_accel_hashes_new\n",
    "\n",
    "old_best_performance = update_dataset(graphLib, accel_dataset, cnn_models_dir, accel_models_dir, \n",
    "    new_graphlib_file, accel_dataset_file_trained, performance_weights, save_dataset=False)\n",
    "\n",
    "# Get entire dataset in embedding space\n",
    "cnn_embeddings = []\n",
    "for graph in graphLib.library:\n",
    "    cnn_embeddings.append(graph.embedding)\n",
    "cnn_embeddings = np.array(cnn_embeddings)\n",
    "\n",
    "min_cnn, max_cnn = np.min(cnn_embeddings, axis=0), np.max(cnn_embeddings, axis=0)\n",
    "min_accel, max_accel = np.min(accel_embeddings, axis=0), np.max(accel_embeddings, axis=0)\n",
    "\n",
    "X_ds = []\n",
    "for cnn_idx in range(cnn_embeddings.shape[0]):\n",
    "    for accel_idx in range(accel_embeddings.shape[0]):\n",
    "        X_ds.append((cnn_embeddings[cnn_idx, :], accel_embeddings[accel_idx, :]))\n",
    "        \n",
    "# Initialize the two-input BOSHNAS model\n",
    "surrogate_model = BOSHCODE(input_dim1=cnn_embeddings.shape[1],\n",
    "                          input_dim2=accel_embeddings.shape[1],\n",
    "                          bounds1=(min_cnn, max_cnn),\n",
    "                          bounds2=(min_accel, max_accel),\n",
    "                          trust_region=False,\n",
    "                          second_order=True,\n",
    "                          parallel=True,\n",
    "                          model_aleatoric=True,\n",
    "                          save_path='./surrogate_model/',\n",
    "                          pretrained=False)\n",
    "\n",
    "# Get initial dataset after finetuning num_init models\n",
    "X_cnn, X_accel, y = convert_to_tabular(accel_dataset, graphLib, performance_weights)\n",
    "max_loss = np.amax(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerator hash: 81a9133de2e7f24508d1459f076c42072492e2280088eb77c0167d1d209c3297\n",
      "CNN hash: e3a979c95f7b0716b4dc7b5f763e6f11ea659d350caff793e75352fe7791441f\n",
      "CNN neighbor: None\n"
     ]
    }
   ],
   "source": [
    "# Get next queries\n",
    "query_indices = surrogate_model.get_queries(x=X_ds, k=1, explore_type='ucb', use_al=False) \n",
    "\n",
    "# Run queries\n",
    "for i in set(query_indices):\n",
    "    accel_hash = accel_hashes[i]\n",
    "    accel_emb = accel_dataset[accel_hash]['accel_emb']\n",
    "\n",
    "    cnn_model, _ = graphLib.get_graph(model_hash=accel_dataset[accel_hash]['cnn_hash'])\n",
    "    chosen_neighbor_hash = get_neighbor_hash(cnn_model, trained_cnn_hashes)\n",
    "    \n",
    "    print(f'Accelerator hash: {accel_hash}\\nCNN hash: {cnn_model.hash}\\nCNN neighbor: {chosen_neighbor_hash}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnnbench [~/.conda/envs/cnnbench/]",
   "language": "python",
   "name": "conda_cnnbench"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
