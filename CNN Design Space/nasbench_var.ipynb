{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"nasbench_var.ipynb","provenance":[{"file_id":"https://github.com/google-research/nasbench/blob/master/NASBench.ipynb","timestamp":1608040131043}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"vl1oLYux3FhJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609240997767,"user_tz":-330,"elapsed":67954,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"08476644503706754908"}},"outputId":"88c68775-3a99-4448-fc15-cde89d4f70eb"},"source":["# This code was written in TF 1.12 but should be supported all the way through\n","# TF 1.15. Untested in TF 2.0+.\n","%tensorflow_version 1.x\n","\n","# Download the raw data (only 108 epoch data points, for full dataset,\n","# uncomment the second line for nasbench_full.tfrecord).\n","\n","!curl -O https://storage.googleapis.com/nasbench/nasbench_only108.tfrecord\n","# !curl -O https://storage.googleapis.com/nasbench/nasbench_full.tfrecord\n","\n","# Clone and install the code and dependencies.\n","\n","!git clone https://github.com/google-research/nasbench\n","!pip install ./nasbench\n","\n","# Initialize the NASBench object which parses the raw data into memory (this\n","# should only be run once as it takes up to a few minutes).\n","from nasbench import api\n","\n","import sys\n","if './nasbench/nasbench/' not in sys.path:\n","  sys.path.append('./nasbench/nasbench/')\n","\n","from api import *\n","\n","dataset_file = 'nasbench_only108.tfrecord'\n","\n","# Use nasbench_full.tfrecord for full dataset (run download command above).\n","nasbench = api.NASBench(dataset_file)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  498M  100  498M    0     0   108M      0  0:00:04  0:00:04 --:--:--  108M\n","Cloning into 'nasbench'...\n","remote: Enumerating objects: 96, done.\u001b[K\n","remote: Total 96 (delta 0), reused 0 (delta 0), pack-reused 96\u001b[K\n","Unpacking objects: 100% (96/96), done.\n","Processing ./nasbench\n","Requirement already satisfied: tensorflow>=1.12.0 in /tensorflow-1.15.2/python3.6 (from nasbench==1.0) (1.15.2)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12.0->nasbench==1.0) (1.1.2)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12.0->nasbench==1.0) (0.10.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12.0->nasbench==1.0) (0.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12.0->nasbench==1.0) (3.3.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12.0->nasbench==1.0) (0.8.1)\n","Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /tensorflow-1.15.2/python3.6 (from tensorflow>=1.12.0->nasbench==1.0) (1.15.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12.0->nasbench==1.0) (1.12.1)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12.0->nasbench==1.0) (1.1.0)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12.0->nasbench==1.0) (1.19.4)\n","Collecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12.0->nasbench==1.0) (1.32.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12.0->nasbench==1.0) (1.15.0)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12.0->nasbench==1.0) (0.36.2)\n","Requirement already satisfied: keras-applications>=1.0.8 in /tensorflow-1.15.2/python3.6 (from tensorflow>=1.12.0->nasbench==1.0) (1.0.8)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12.0->nasbench==1.0) (3.12.4)\n","Requirement already satisfied: tensorflow-estimator==1.15.1 in /tensorflow-1.15.2/python3.6 (from tensorflow>=1.12.0->nasbench==1.0) (1.15.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.12.0->nasbench==1.0) (3.3.3)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.12.0->nasbench==1.0) (1.0.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.12.0->nasbench==1.0) (51.0.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow>=1.12.0->nasbench==1.0) (2.10.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow>=1.12.0->nasbench==1.0) (3.3.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow>=1.12.0->nasbench==1.0) (3.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow>=1.12.0->nasbench==1.0) (3.7.4.3)\n","Building wheels for collected packages: nasbench, gast\n","  Building wheel for nasbench (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nasbench: filename=nasbench-1.0-cp36-none-any.whl size=46789 sha256=6f34e9d5bbde255c35345c23bbdbe54b49d3805d0eb64c098c9d98fb9141f721\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-tsmed7bj/wheels/4b/19/99/1d5fdfe30f8b16fab91e900808f4f7e5adc38e602c84970ad5\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=83aa8dee8bfe181786d2a9f46bac39139e511f1aaaa61e6a40e724c084bda968\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built nasbench gast\n","Installing collected packages: nasbench, gast\n","  Found existing installation: gast 0.3.3\n","    Uninstalling gast-0.3.3:\n","      Successfully uninstalled gast-0.3.3\n","Successfully installed gast-0.2.2 nasbench-1.0\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/nasbench/lib/training_time.py:130: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/nasbench/lib/training_time.py:174: The name tf.train.CheckpointSaverListener is deprecated. Please use tf.estimator.CheckpointSaverListener instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/nasbench/lib/evaluate.py:30: The name tf.train.NanLossDuringTrainingError is deprecated. Please use tf.estimator.NanLossDuringTrainingError instead.\n","\n","Loading dataset from file... This may take a few minutes...\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/nasbench/api.py:146: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use eager execution and: \n","`tf.data.TFRecordDataset(path)`\n","Loaded dataset in 51 seconds\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oFhFRmck7NzM","executionInfo":{"status":"ok","timestamp":1609242754484,"user_tz":-330,"elapsed":1043,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"08476644503706754908"}}},"source":["# Standard imports\n","import copy\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","from scipy.sparse import coo_matrix as compress\n","\n","# Useful constants\n","INPUT = 'input'\n","OUTPUT = 'output'\n","CONV3X3 = 'conv3x3-bn-relu'\n","CONV1X1 = 'conv1x1-bn-relu'\n","MAXPOOL3X3 = 'maxpool3x3'\n","NUM_VERTICES = 7\n","MAX_EDGES = 9\n","EDGE_SPOTS = NUM_VERTICES * (NUM_VERTICES - 1) / 2   # Upper triangular matrix\n","OP_SPOTS = NUM_VERTICES - 2   # Input/output vertices are fixed\n","ALLOWED_OPS = [CONV3X3, CONV1X1, MAXPOOL3X3]\n","ALLOWED_EDGES = [0, 1]   # Binary adjacency matrix"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i9SOVEvxrwwa","executionInfo":{"status":"ok","timestamp":1609241828497,"user_tz":-330,"elapsed":279341,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"08476644503706754908"}},"outputId":"6a5c276a-5404-4d88-f90c-8d2bca23d032"},"source":["count = 0\r\n","max_count = 423624 * 3\r\n","\r\n","adjacency_list = []\r\n","operations_list = []\r\n","\r\n","print('Generating dataset')\r\n","for serialized_row in tf.python_io.tf_record_iterator(dataset_file):\r\n","\r\n","  count += 1\r\n","  print('\\rCompleted: %0.2f%%' % (count/(max_count)*100), end='')\r\n","\r\n","  # Take only unique adjacency matrices\r\n","  if count % 3 != 0: continue\r\n","\r\n","  # Parse the data from the data file.\r\n","  module_hash, epochs, raw_adjacency, raw_operations, raw_metrics = (\r\n","      json.loads(serialized_row.decode('utf-8')))\r\n","\r\n","  dim = int(np.sqrt(len(raw_adjacency)))\r\n","  adjacency = np.array([int(e) for e in list(raw_adjacency)], dtype=np.int8)\r\n","  adjacency = np.reshape(adjacency, (dim, dim))\r\n","  operations = raw_operations.split(',')\r\n","  metrics = model_metrics_pb2.ModelMetrics.FromString(\r\n","      base64.b64decode(raw_metrics))\r\n","  \r\n","  adjacency_list.append(adjacency)\r\n","  operations_list.append(operations)\r\n","\r\n","  # Evaluation statistics at the end of training\r\n","  '''\r\n","  final_evaluation = metrics.evaluation_data[2]\r\n","  training_time = final_evaluation.training_time\r\n","  train_accuracy = final_evaluation.train_accuracy\r\n","  validation_accuracy = (\r\n","      final_evaluation.validation_accuracy)\r\n","  test_accuracy = final_evaluation.test_accuracy\r\n","  trainable_params = metrics.trainable_parameters\r\n","\r\n","  print(f'Module {count//3+1}.{count%3+1} \\nAdjacency matrix: \\n{adjacency} \\nOperations: {operations} \\nTrainable parameters: {trainable_params}')\r\n","  print(f'Train Accuracy: {train_accuracy} \\nValidation Accuracy: {validation_accuracy} \\nTest Accuracy: {test_accuracy}\\n')\r\n","  '''"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Generating dataset\n","Completed: 100.00%"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PQy0zMtsSajm","executionInfo":{"status":"ok","timestamp":1609243087142,"user_tz":-330,"elapsed":131628,"user":{"displayName":"Shikhar Tuli","photoUrl":"","userId":"08476644503706754908"}},"outputId":"e8ee15ce-b03e-4a39-b9c7-f93a6796b839"},"source":["ordered_operations = [INPUT, OUTPUT, MAXPOOL3X3, CONV1X1, CONV3X3]\r\n","\r\n","weighted_adjacency_list = copy.deepcopy(adjacency_list)\r\n","compressed_weighted_list = []\r\n","\r\n","count = 0\r\n","\r\n","print('Augmenting dataset')\r\n","for i in range(len(weighted_adjacency_list)):\r\n","\r\n","  count += 1\r\n","  print('\\rCompleted: %0.2f%%' % (count/(max_count/3)*100), end='')\r\n","\r\n","  vertices = np.shape(weighted_adjacency_list[i])[0]\r\n","\r\n","  for v in range(vertices):\r\n","    weighted_adjacency_list[i][v, :] *= (ordered_operations.index(operations_list[i][v]) + 1)\r\n","\r\n","  compressed_weighted_list.append(compress(weighted_adjacency_list[i]))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Augmenting dataset\n","Completed: 100.00%"],"name":"stdout"}]}]}